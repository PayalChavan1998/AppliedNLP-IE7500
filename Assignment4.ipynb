{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d589a4-48f2-42a2-ad99-b6ea82055220",
   "metadata": {},
   "source": [
    "## Assignment 4 - Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e6261-cdd5-4ba4-ad5e-e63f8919b70d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The primary objectives of this assignment are as follows:\n",
    "\n",
    "1. Develop a custom transformer-based machine translation model.\n",
    "2. Implement machine translation using pre-trained transformer models tailored to the provided dataset.\n",
    "3. Conduct a comprehensive comparative analysis of the output generated by the custom transformer and pre-trained transformer models, with the aid of BLEU metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841f29a-9945-4b56-ac07-210addd614b5",
   "metadata": {},
   "source": [
    "### TASK 1: Dataset Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59a6d1-a3bc-4869-a515-8251925ca2fe",
   "metadata": {},
   "source": [
    "For our machine translation assignment, we chose an appropriate dataset for translating English to Marathi from the website https://www.manythings.org/anki/. This dataset will be instrumental in training and evaluating our translation models, ensuring they accurately capture the nuances of both languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018292c3-03a8-4c2d-8d76-d4240c7dfca7",
   "metadata": {},
   "source": [
    "Includes libraries for PyTorch operations (torch and torch.nn), optimizers (Adam), learning rate scheduling (StepLR), and utilities for tokenization and dataset management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262d9fa2-3b06-4108-b938-d2c3b9a3cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/payalchavan/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fccd1-f783-40b1-ab18-0745b0ee3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "file_path = '/Users/payalchavan/Documents/Applied_NLP/Assignment4/mar-eng/mar.txt'  \n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            data.append({\"english\": str(parts[0]), \"marathi\": str(parts[1])})\n",
    "\n",
    "translation_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3306ff9-5a42-43ca-8416-10aa1fd7cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "translation_df = translation_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105ac81-eab9-40c7-8aa8-d7af28bef424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 2500 records\n",
    "translation_df = translation_df.head(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba71ce-6f52-4547-9279-8996297644ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to CSV file\n",
    "output_csv_path = 'sample_2500_records.csv'\n",
    "translation_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d640f9c-c3ff-47c0-9f65-48b484d74f18",
   "metadata": {},
   "source": [
    "#### Start running your code from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0779c4c1-c313-49d4-8411-2b10b76c5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved .csv file\n",
    "translation_df = pd.read_csv(r'sample_2500_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a914314f-2bd7-4004-b163-4f147c647f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your existing .csv file \n",
    "input_csv_path = 'sample_2500_records.csv' \n",
    "\n",
    "# Path where you want to save the .txt file \n",
    "output_txt_path = 'sample_2500_records.txt'\n",
    "\n",
    "# Export to .txt file \n",
    "translation_df.to_csv(output_txt_path, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a340da46-c874-4ce6-8751-d669ccd2dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   english  2500 non-null   object\n",
      " 1   marathi  2500 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 39.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>marathi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>Tom doesn't want to go home.</td>\n",
       "      <td>टॉमला घरी जायचं नाहीये.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>I made a mistake.</td>\n",
       "      <td>मी चूक केली.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>I'll wait until October.</td>\n",
       "      <td>मी ऑक्टोबरपर्यंत थांबेन.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>What do you call this in French?</td>\n",
       "      <td>याला फ्रेंचमध्ये काय म्हणतात?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Tom didn't realize that we could do that.</td>\n",
       "      <td>आपण तसं करू शकू याची टॉमला जाणीव झाली नाही.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english  \\\n",
       "2495               Tom doesn't want to go home.   \n",
       "2496                          I made a mistake.   \n",
       "2497                   I'll wait until October.   \n",
       "2498           What do you call this in French?   \n",
       "2499  Tom didn't realize that we could do that.   \n",
       "\n",
       "                                          marathi  \n",
       "2495                      टॉमला घरी जायचं नाहीये.  \n",
       "2496                                 मी चूक केली.  \n",
       "2497                     मी ऑक्टोबरपर्यंत थांबेन.  \n",
       "2498                याला फ्रेंचमध्ये काय म्हणतात?  \n",
       "2499  आपण तसं करू शकू याची टॉमला जाणीव झाली नाही.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the basic info and display the records\n",
    "translation_df.info()\n",
    "translation_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df05ef-aae9-4989-8657-133cbc1759fd",
   "metadata": {},
   "source": [
    "- We used an English-Marathi parallel corpus for training and evaluation. \n",
    "- The dataset consists of sentence pairs, with English sentences and their corresponding Marathi translations.\n",
    "\n",
    "The English-to-Marathi dataset chosen for this task forms a fundamental base for businesses targeting the Indian market. However, with only 2500 sentence pairs, the dataset is relatively small, limiting the effectiveness of a custom-built model due to its lack of diverse language patterns. For businesses striving to provide high-quality machine translation services, particularly in regions with intricate languages like Marathi, it is crucial to source a more extensive dataset. Investing in richer datasets will lead to more accurate and nuanced translations, thereby enhancing customer engagement and improving localization quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d5b8b-8b42-42b9-836d-81103ea67825",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657a193-07e8-4b4d-b043-b239a9afaa61",
   "metadata": {},
   "source": [
    "This preprocessing step will:\n",
    "1. Normalize and clean the English text.\n",
    "2. Remove non-Devanagari characters and license information from Marathi text.\n",
    "3. Filter out pairs that are too long (default max length is 10 words).\n",
    "4. Provide some basic statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d4d994-a221-45d6-afc6-4504f434c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s: str) -> str:\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f62e5714-7ae6-46c8-bebf-49670dd12fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "   #s = re.sub(r'[.!?]+', '', s)  # Remove full stops, exclamation marks, and question marks\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)  # Remove other punctuation\n",
    "    s = re.sub(r'\\s+', ' ', s)  # Replace multiple spaces with a single space\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665c664d-1416-4206-8a2b-81e626d177b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_marathi(s: str) -> str:\n",
    "    # Remove the CC-BY 2.0 license information\n",
    "    s = re.sub(r'CC-BY 2\\.0.*$', '', s)\n",
    "    # Remove any non-Devanagari characters and punctuation\n",
    "    s = re.sub(r'[^\\u0900-\\u097F\\s]', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)  # Replace multiple spaces with a single space\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e20c11f-82f3-4b06-82da-fabb98b4a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str) -> List[Tuple[str, str]]:\n",
    "    pairs = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                english = normalize_string(parts[0])\n",
    "                marathi = preprocess_marathi(parts[1])\n",
    "                if english and marathi:  # Ensure both parts are non-empty\n",
    "                    pairs.append((english, marathi))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad984bbf-5cf0-4a5c-94db-c59ce84e079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs: List[Tuple[str, str]], max_length: int) -> List[Tuple[str, str]]:\n",
    "    return [(eng, mar) for eng, mar in pairs \n",
    "            if len(eng.split()) <= max_length and len(mar.split()) <= max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "503d2206-c2fc-4549-a9f5-baaf050fb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main preprocessing function\n",
    "def preprocess_data(filename: str, max_length: int = 10) -> List[Tuple[str, str]]:\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    pairs = load_data(filename)\n",
    "    print(f\"Total pairs loaded: {len(pairs)}\")\n",
    "    \n",
    "    filtered_pairs = filter_pairs(pairs, max_length)\n",
    "    print(f\"Pairs after filtering (max length {max_length}): {len(filtered_pairs)}\")\n",
    "    \n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16763df8-b0b8-4867-a891-a8ecddc7ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Total pairs loaded: 2500\n",
      "Pairs after filtering (max length 10): 2458\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "filename = 'sample_2500_records.txt'\n",
    "preprocessed_data = preprocess_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25ba5f20-ced6-4ac1-99f0-79a9f706fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example pairs:\n",
      "Pair 1:\n",
      "English: tom doesnt want to go home\n",
      "Marathi: टॉमला घरी जायचं नाहीये\n",
      "\n",
      "Pair 2:\n",
      "English: i made a mistake\n",
      "Marathi: मी चूक केली\n",
      "\n",
      "Pair 3:\n",
      "English: ill wait until october\n",
      "Marathi: मी ऑक्टोबरपर्यंत थांबेन\n",
      "\n",
      "Pair 4:\n",
      "English: what do you call this in french\n",
      "Marathi: याला फ्रेंचमध्ये काय म्हणतात\n",
      "\n",
      "Pair 5:\n",
      "English: tom didnt realize that we could do that\n",
      "Marathi: आपण तसं करू शकू याची टॉमला जाणीव झाली नाही\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some examples\n",
    "print(\"\\nExample pairs:\")\n",
    "for i, (eng, mar) in enumerate(preprocessed_data[-5:]):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"English: {eng}\")\n",
    "    print(f\"Marathi: {mar}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c81ebb63-abde-4242-86bb-c002b98d8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional statistics\n",
    "eng_words = set()\n",
    "mar_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1270132c-2420-4cbe-9934-809a3348f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, mar in preprocessed_data:\n",
    "    eng_words.update(eng.split())\n",
    "    mar_words.update(mar.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "757d88b6-d1d6-4ed1-b75b-eaa491428f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique English words: 1794\n",
      "Total unique Marathi words: 3054\n",
      "Total preprocessed pairs: 2458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique English words: {len(eng_words)}\")\n",
    "print(f\"Total unique Marathi words: {len(mar_words)}\")\n",
    "print(f\"Total preprocessed pairs: {len(preprocessed_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3d75d-2655-4c07-b1b2-325eb2de4183",
   "metadata": {},
   "source": [
    "The Marathi vocabulary is almost double the size of the English vocabulary (3,054 vs. 1,794). This could be due to the morphological richness of Marathi compared to English, where a single word in English might translate into multiple forms in Marathi. The dataset contains 2,458 preprocessed pairs of sentences or phrases. This is a relatively small dataset for training machine translation models, which may affect the performance of a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f166f0aa-623c-4a8e-a682-fab760d82de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume df is the original DataFrame containing the full dataset\n",
    "df_train, df_test = train_test_split(translation_df, test_size=0.2, random_state=42)  # 80-20 split\n",
    "\n",
    "# Convert DataFrames to Dataset format\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f568bc8-0aa1-470e-93dd-ea4ab07b4ef9",
   "metadata": {},
   "source": [
    "We used the train_test_split function from scikit-learn to divide the dataset into training and testing sets in an 80-20 ratio. This ensures our model is trained on 80% of the data and evaluated on the remaining 20%, enhancing model performance and preventing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc647ebb-aab4-4aba-9276-670e4f6b3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer \n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-mr\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad33df-2fe3-48bb-aedb-e00433ee2b93",
   "metadata": {},
   "source": [
    "The \"Helsinki-NLP/opus-mt-en-mr\" model is a neural machine translation model developed by Helsinki-NLP and available on Hugging Face. It translates text from English to Marathi using the Marian NMT framework. The model is trained on the OPUS dataset, which includes a variety of text sources to ensure diverse language patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a881d3f-522b-4385-9322-6b34c7e13281",
   "metadata": {},
   "source": [
    "####  Define the Transformer Architecture\n",
    "\n",
    "The Transformer architecture consists of:\n",
    "\n",
    "__Encoder:__ Processes the source language (English) input.\n",
    "\n",
    "__Decoder:__ Generates the target language (Marathi) output, conditioned on the encoder's representation and previous tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "835e4888-878e-42d9-ae36-bd3f991a1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=0)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = nn.Sequential()  # Assume this is defined elsewhere\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True  # Use batch_first=True to align batch dimension\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # Embed and position-encode inputs\n",
    "        src = self.positional_encoding(self.src_embedding(src))\n",
    "        tgt = self.positional_encoding(self.tgt_embedding(tgt))\n",
    "\n",
    "        # Pass through transformer and project to vocab size\n",
    "        output = self.transformer(\n",
    "            src, tgt, src_mask=src_mask, tgt_mask=tgt_mask, \n",
    "            src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7021c9a-64d6-497e-a7fa-cf804a3a6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the inputs and outputs\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(text) for text in examples[\"english\"]]\n",
    "    targets = [str(text) for text in examples[\"marathi\"]]\n",
    "    \n",
    "    # Tokenize and pad inputs and labels\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    \n",
    "    # Replace pad token in labels with -100 for loss masking\n",
    "    labels = [[label if label != tokenizer.pad_token_id else -100 for label in label_seq] for label_seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f5945ad-37d3-4b58-afe5-55ab9917863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 2000/2000 [00:00<00:00, 9263.50 examples/s]\n",
      "Map: 100%|██████████████████████████| 500/500 [00:00<00:00, 10607.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2845fe-9d65-4143-acd4-ef40368f8eeb",
   "metadata": {},
   "source": [
    "### TASK 2: Custom Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ccd86-21d5-463f-8b71-b013611f28a6",
   "metadata": {},
   "source": [
    "Develop a custom transformer-based machine translation model tailored to the selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1f4a687-14ab-4920-8d43-c85b5e984b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collator to ensure consistent tensor creation\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "        # Only process tensor-compatible fields\n",
    "        tensor_fields = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "        for key in tensor_fields:\n",
    "            values = [torch.tensor(f[key]) for f in features]\n",
    "            batch[key] = torch.stack(values)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8ae47e3-8f02-4f55-bcc1-97092c5c6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "411b174e-718d-43c1-9779-858281012018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: {'input_ids': torch.Size([16, 128]), 'attention_mask': torch.Size([16, 128]), 'labels': torch.Size([16, 128])}\n",
      "Test batch: {'input_ids': torch.Size([16, 128]), 'attention_mask': torch.Size([16, 128]), 'labels': torch.Size([16, 128])}\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for train and test data\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "# Display a sample batch from the training data to verify tensor shapes\n",
    "for batch in train_dataloader:\n",
    "    print(\"Training batch:\", {k: v.shape for k, v in batch.items()})\n",
    "    break\n",
    "\n",
    "# Display a sample batch from the test data to verify tensor shapes\n",
    "for batch in test_dataloader:\n",
    "    print(\"Test batch:\", {k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe3bd98b-1d0d-4b41-9937-d0bb709913c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 61674\n"
     ]
    }
   ],
   "source": [
    "# Print the vocabulary size\n",
    "print(\"Vocabulary Size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e0de8b8-4afa-463a-8e40-3ee256e98051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Embedding Shape: torch.Size([61674, 512])\n",
      "Target Embedding Shape: torch.Size([61674, 512])\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model parameters\n",
    "src_vocab_size = 61674\n",
    "tgt_vocab_size = 61674\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "# Print the shape of the embedding weights\n",
    "print(\"Source Embedding Shape:\", model.src_embedding.weight.shape)\n",
    "print(\"Target Embedding Shape:\", model.tgt_embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e620c60d-da03-45ca-bc25-7653608d2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Pad Token ID: 0\n"
     ]
    }
   ],
   "source": [
    "# Reset the padding token to a safer, lower ID\n",
    "tokenizer.pad_token_id = 0  # Use 0 if it's not in conflict with other tokens\n",
    "print(\"Updated Pad Token ID:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cabf7b48-7b1b-4db1-afdf-62f1ff7536ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.7044\n",
      "Epoch 1, Test Loss: 4.0496\n",
      "Epoch 2, Loss: 4.0458\n",
      "Epoch 2, Test Loss: 4.0404\n",
      "Epoch 3, Loss: 4.0407\n",
      "Epoch 3, Test Loss: 4.0360\n",
      "Epoch 4, Loss: 4.0385\n",
      "Epoch 4, Test Loss: 4.0333\n",
      "Epoch 5, Loss: 4.0349\n",
      "Epoch 5, Test Loss: 4.0322\n"
     ]
    }
   ],
   "source": [
    "# Import torch.nn.functional for cross_entropy\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "src_vocab_size = 62000  # Adjusted vocab size\n",
    "tgt_vocab_size = 62000\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):  # Use train_dataloader for training\n",
    "        print(f\"{i}\", end='\\r', flush=True)\n",
    "        # Prepare src and tgt\n",
    "        src = batch['input_ids'].to(device)\n",
    "        tgt = batch['labels'].to(device)\n",
    "\n",
    "        # Adjust tgt_input and tgt_output\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:].contiguous()\n",
    "\n",
    "        # Replace -100 with pad token ID in tgt_input\n",
    "        tgt_input = torch.where(tgt_input == -100, torch.tensor(tokenizer.pad_token_id).to(device), tgt_input)\n",
    "\n",
    "        # Generate masks and padding masks\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        src_key_padding_mask = (src == tokenizer.pad_token_id).to(device)\n",
    "        tgt_key_padding_mask = (tgt_input == tokenizer.pad_token_id).to(device)\n",
    "\n",
    "        # Forward pass with consistent batch size and sequence dimension\n",
    "        try:\n",
    "            logits = model(\n",
    "                src, tgt_input, tgt_mask=tgt_mask, \n",
    "                src_key_padding_mask=src_key_padding_mask, \n",
    "                tgt_key_padding_mask=tgt_key_padding_mask\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError in forward pass: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss using F.cross_entropy\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, tgt_vocab_size),\n",
    "            tgt_output.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the test set after each epoch\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:  # Use test_dataloader for evaluation\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "\n",
    "            # Adjust tgt_input and tgt_output\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:].contiguous()\n",
    "\n",
    "            # Replace -100 with pad token ID in tgt_input\n",
    "            tgt_input = torch.where(tgt_input == -100, torch.tensor(tokenizer.pad_token_id).to(device), tgt_input)\n",
    "\n",
    "            # Generate masks and padding masks\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            src_key_padding_mask = (src == tokenizer.pad_token_id).to(device)\n",
    "            tgt_key_padding_mask = (tgt_input == tokenizer.pad_token_id).to(device)\n",
    "\n",
    "            try:\n",
    "                logits = model(\n",
    "                    src, tgt_input, tgt_mask=tgt_mask, \n",
    "                    src_key_padding_mask=src_key_padding_mask, \n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError in evaluation forward pass: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the test loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, tgt_vocab_size),\n",
    "                tgt_output.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Test Loss: {average_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667c33b-be65-4046-8dbd-21de40389529",
   "metadata": {},
   "source": [
    "Over the course of five epochs, both training and test losses decrease gradually but plateau around epoch 3. The training loss starts at 4.7044 and drops to around 4.0349 by epoch 5, while the test loss starts at 4.0496 and decreases only slightly to 4.0322. By epoch 3, both training and test losses have nearly plateaued (training loss at ~4.0407 and test loss at ~4.0360). This suggests that further training may not yield significant improvements without changes in the model architecture or training strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63ddeae7-26a8-49e6-b0da-62d835e1b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the custom model\n",
    "torch.save(model.state_dict(), 'transformer_translation_model_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87fbaceb-495b-402f-8bf7-2e95bf1c41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in both .pth and .pkl formats.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Save the model state_dict (as you did before)\n",
    "torch.save(model.state_dict(), 'transformer_translation_model_1.pth')\n",
    "\n",
    "# Save the entire model using pickle\n",
    "with open('transformer_translation_model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved in both .pth and .pkl formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b313ba8-2296-4971-99dd-cdea2953cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from .pkl format.\n",
      "English: Who did you meet?\n",
      "Reference Marathi: तुम्ही कोणाला भेटलात?\n",
      "Custom Model Translation: did you meet?\n",
      "\n",
      "English: We named the dog Cookie.\n",
      "Reference Marathi: कुत्र्याचं नाव आपण कुकी ठेवलं.\n",
      "Custom Model Translation: named the dog Cookie.\n",
      "\n",
      "English: \"When do you get up?\" \"At 8 in the morning.\"\n",
      "Reference Marathi: \"तू किती वाजता उठतेस?\" \"सकाळी ८ वाजता.\"\n",
      "Custom Model Translation: When do you get up?\" \"At 8 in the morning.\"\n",
      "\n",
      "English: Sit down there.\n",
      "Reference Marathi: तिथे खाली बसा.\n",
      "Custom Model Translation: down there.\n",
      "\n",
      "English: You decide.\n",
      "Reference Marathi: तुम्हीच ठरवा.\n",
      "Custom Model Translation: decide.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-mr\"  # Use the same tokenizer as before\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Assuming `tokenized_test_dataset` is already created from `test_dataset` with preprocessing\n",
    "# Custom DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# Define function to load custom model\n",
    "def load_custom_model(pth_path, pkl_path, src_vocab_size, tgt_vocab_size):\n",
    "    # Try loading from .pkl format first\n",
    "    try:\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(\"Model loaded successfully from .pkl format.\")\n",
    "    except FileNotFoundError:\n",
    "        # If .pkl file not found, load from .pth state dictionary\n",
    "        model = TransformerModel(src_vocab_size, tgt_vocab_size)\n",
    "        model.load_state_dict(torch.load(pth_path))\n",
    "        print(\"Model loaded successfully from .pth format.\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the model (update the paths as necessary)\n",
    "src_vocab_size = 62000  # Adjust as needed\n",
    "tgt_vocab_size = 62000  # Adjust as needed\n",
    "pth_path = 'transformer_translation_model_1.pth'\n",
    "pkl_path = 'transformer_translation_model_1.pkl'\n",
    "model = load_custom_model(pth_path, pkl_path, src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# Custom translation function to handle sequential generation (one token at a time)\n",
    "def generate_translation_custom(model, input_ids, max_length=50):\n",
    "    generated_ids = input_ids\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated_ids, tgt=generated_ids)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "        \n",
    "        # Select the most likely next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "        \n",
    "        # Stop if all sequences have generated an EOS token\n",
    "        if torch.all(next_token_id == tokenizer.eos_token_id):\n",
    "            break\n",
    "\n",
    "    return generated_ids[:, 1:]  # Skip the initial token\n",
    "\n",
    "# Generate translations for each test batch\n",
    "custom_translations = []\n",
    "input_sentences = []\n",
    "reference_sentences = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    output_ids = generate_translation_custom(model, input_ids)\n",
    "    translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    custom_translations.append(translation)\n",
    "\n",
    "    # Retrieve the original sentences for reference\n",
    "    original_sentence = test_dataset[i]['english']\n",
    "    reference_translation = test_dataset[i]['marathi']\n",
    "    input_sentences.append(original_sentence)\n",
    "    reference_sentences.append(reference_translation)\n",
    "\n",
    "    # Print a few example translations\n",
    "    if i < 5:  # Display only the first 5 examples\n",
    "        print(f\"English: {original_sentence}\")\n",
    "        print(f\"Reference Marathi: {reference_translation}\")\n",
    "        print(f\"Custom Model Translation: {translation}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89716366-3742-48d8-b7f2-e8252d180d49",
   "metadata": {},
   "source": [
    "Translation Accuracy:\n",
    "- Omissions: The custom model frequently omits crucial parts of sentences. For example, \"Who did you meet?\" is translated to \"did you meet?\" and \"Sit down there\" to \"down there.\" These omissions significantly affect the clarity and completeness of the translations.\n",
    "- Partial Accuracy: In the case of \"We named the dog Cookie,\" the custom model translates it accurately, maintaining the sentence structure and meaning.\n",
    "- The custom model shows potential in handling straightforward and simple phrases but struggles with sentence completeness and contextual accuracy.\n",
    "- There is a need for more comprehensive training and fine-tuning, specifically focusing on maintaining the full structure of the original sentences and improving contextual understanding.\n",
    "\n",
    "The custom model demonstrates an initial capability to translate basic phrases, yet it requires significant enhancements to handle more complex sentence structures and maintain full sentence integrity. Addressing these issues through additional training with a larger, more diverse dataset and refining the translation algorithms could improve the model's performance, leading to more accurate and contextually appropriate translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb82474-7071-44e2-a031-6b67e1bf3fb2",
   "metadata": {},
   "source": [
    "### TASK 3: Pre-trained Transformer Usage\n",
    "Utilize pre-trained transformer models for the same dataset, optimizing their performance for machine translation.\n",
    "(This section would include loading and fine-tuning a pre-trained model, such as Helsinki-NLP/opus-mt-en-mr. Code specifics would depend on the pre-trained model setup, similar to above but simplified given pre-trained weights.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1293e093-da17-4938-a660-d538e8644886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Who did you meet?\n",
      "Reference Marathi: तुम्ही कोणाला भेटलात?\n",
      "Pre-trained Model Translation: तू कोणाला भेटलास?\n",
      "\n",
      "English: We named the dog Cookie.\n",
      "Reference Marathi: कुत्र्याचं नाव आपण कुकी ठेवलं.\n",
      "Pre-trained Model Translation: आम्ही आमच्या कुत्र्याचं नाव 'कुकी' ठेवलं.\n",
      "\n",
      "English: \"When do you get up?\" \"At 8 in the morning.\"\n",
      "Reference Marathi: \"तू किती वाजता उठतेस?\" \"सकाळी ८ वाजता.\"\n",
      "Pre-trained Model Translation: \"तू किती वाजता उठतोस?\" \"नाही.\"\n",
      "\n",
      "English: Sit down there.\n",
      "Reference Marathi: तिथे खाली बसा.\n",
      "Pre-trained Model Translation: खाली बस.\n",
      "\n",
      "English: You decide.\n",
      "Reference Marathi: तुम्हीच ठरवा.\n",
      "Pre-trained Model Translation: तू ठरव.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-mr\"\n",
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Assuming `tokenized_test_dataset` is already created from `test_dataset` with preprocessing\n",
    "# Custom DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# Generate translations using the pre-trained model\n",
    "pretrained_translations = []\n",
    "input_sentences = []\n",
    "reference_sentences = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        translated_outputs = pretrained_model.generate(input_ids, max_length=50)\n",
    "        \n",
    "    # Decode translations\n",
    "    translation = pretrained_tokenizer.decode(translated_outputs[0], skip_special_tokens=True)\n",
    "    pretrained_translations.append(translation)\n",
    "    \n",
    "    # Retrieve the original sentences for reference\n",
    "    original_sentence = test_dataset[i]['english']\n",
    "    reference_translation = test_dataset[i]['marathi']\n",
    "    input_sentences.append(original_sentence)\n",
    "    reference_sentences.append(reference_translation)\n",
    "    \n",
    "    # Print a few example translations\n",
    "    if i < 5:  # Display only the first 5 examples\n",
    "        print(f\"English: {original_sentence}\")\n",
    "        print(f\"Reference Marathi: {reference_translation}\")\n",
    "        print(f\"Pre-trained Model Translation: {translation}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ac9a8-645b-4c2f-b2ef-c53620357939",
   "metadata": {},
   "source": [
    "General Observations:\n",
    "\n",
    "- Consistency: The model shows consistency in translating basic sentences but needs refinement in handling nuanced expressions and maintaining the intended tone.\n",
    "\n",
    "- Potential for Improvement: There is room for enhancing the model's ability to translate dialogues and complex sentences more accurately, possibly through fine-tuning on more diverse datasets that include conversational contexts.\n",
    "\n",
    "The pre-trained model demonstrates a strong understanding of basic sentence structures and proper nouns but faces challenges with complex dialogues and maintaining formality. These insights suggest that further fine-tuning and training with additional context-specific data could improve the model's performance and ensure more accurate and contextually appropriate translations. This evaluation provides a clear direction for enhancing the model and tailoring it to better meet specific translation needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ade8a5-a856-4436-8fda-9dcf7d8c599e",
   "metadata": {},
   "source": [
    "### TASK 4: Comparative Analysis\n",
    "Perform a detailed comparative study to assess the output generated by the custom transformer and pre-trained transformer models. Evaluate these outputs using BLEU metrics to quantify translation quality and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56eea4f9-374a-487a-be0e-7322e2673009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (src_embedding): Embedding(62000, 512, padding_idx=0)\n",
       "  (tgt_embedding): Embedding(62000, 512, padding_idx=0)\n",
       "  (positional_encoding): Sequential()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=62000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved custom model\n",
    "model.load_state_dict(torch.load('transformer_translation_model_1.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b3f1aad-c927-41bd-a864-8cc57adf2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from .pkl format.\n",
      "English: Who did you meet?\n",
      "Reference Marathi: तुम्ही कोणाला भेटलात?\n",
      "Custom Model Translation: did you meet?\n",
      "Pre-trained Model Translation: तू कोणाला भेटलास?\n",
      "\n",
      "English: We named the dog Cookie.\n",
      "Reference Marathi: कुत्र्याचं नाव आपण कुकी ठेवलं.\n",
      "Custom Model Translation: named the dog Cookie.\n",
      "Pre-trained Model Translation: आम्ही आमच्या कुत्र्याचं नाव 'कुकी' ठेवलं.\n",
      "\n",
      "English: \"When do you get up?\" \"At 8 in the morning.\"\n",
      "Reference Marathi: \"तू किती वाजता उठतेस?\" \"सकाळी ८ वाजता.\"\n",
      "Custom Model Translation: When do you get up?\" \"At 8 in the morning.\"\n",
      "Pre-trained Model Translation: \"तू किती वाजता उठतोस?\" \"नाही.\"\n",
      "\n",
      "English: Sit down there.\n",
      "Reference Marathi: तिथे खाली बसा.\n",
      "Custom Model Translation: down there.\n",
      "Pre-trained Model Translation: खाली बस.\n",
      "\n",
      "English: You decide.\n",
      "Reference Marathi: तुम्हीच ठरवा.\n",
      "Custom Model Translation: decide.\n",
      "Pre-trained Model Translation: तू ठरव.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-mr\"  # Use the same tokenizer as before\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Assuming `tokenized_test_dataset` is already created from `test_dataset` with preprocessing\n",
    "# Custom DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# Define function to load custom model\n",
    "def load_custom_model(pth_path, pkl_path, src_vocab_size, tgt_vocab_size):\n",
    "    # Try loading from .pkl format first\n",
    "    try:\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(\"Model loaded successfully from .pkl format.\")\n",
    "    except FileNotFoundError:\n",
    "        # If .pkl file not found, load from .pth state dictionary\n",
    "        model = TransformerModel(src_vocab_size, tgt_vocab_size)\n",
    "        model.load_state_dict(torch.load(pth_path))\n",
    "        print(\"Model loaded successfully from .pth format.\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the model (update the paths as necessary)\n",
    "src_vocab_size = 62000  # Adjust as needed\n",
    "tgt_vocab_size = 62000  # Adjust as needed\n",
    "pth_path = 'transformer_translation_model_1.pth'\n",
    "pkl_path = 'transformer_translation_model_1.pkl'\n",
    "model = load_custom_model(pth_path, pkl_path, src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# Custom translation function to handle sequential generation (one token at a time)\n",
    "def generate_translation_custom(model, input_ids, max_length=50):\n",
    "    generated_ids = input_ids\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated_ids, tgt=generated_ids)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "        \n",
    "        # Select the most likely next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "        \n",
    "        # Stop if all sequences have generated an EOS token\n",
    "        if torch.all(next_token_id == tokenizer.eos_token_id):\n",
    "            break\n",
    "\n",
    "    return generated_ids[:, 1:]  # Skip the initial token\n",
    "\n",
    "# Generate translations for each test batch\n",
    "custom_translations = []\n",
    "input_sentences = []\n",
    "reference_sentences = []\n",
    "pretrained_translations = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    output_ids = generate_translation_custom(model, input_ids)\n",
    "    translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    custom_translations.append(translation)\n",
    "\n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        translated_outputs = pretrained_model.generate(input_ids, max_length=50)\n",
    "\n",
    "    # Decode translations\n",
    "    pretrained_translation = pretrained_tokenizer.decode(translated_outputs[0], skip_special_tokens=True)\n",
    "    pretrained_translations.append(pretrained_translation)\n",
    "\n",
    "    # Retrieve the original sentences for reference\n",
    "    original_sentence = test_dataset[i]['english']\n",
    "    reference_translation = test_dataset[i]['marathi']\n",
    "    input_sentences.append(original_sentence)\n",
    "    reference_sentences.append(reference_translation)\n",
    "\n",
    "    # Print a few example translations\n",
    "    if i < 5:  # Display only the first 5 examples\n",
    "        print(f\"English: {original_sentence}\")\n",
    "        print(f\"Reference Marathi: {reference_translation}\")\n",
    "        print(f\"Custom Model Translation: {translation}\")\n",
    "        print(f\"Pre-trained Model Translation: {pretrained_translation}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de5f9e-d15d-468e-a08c-5f130882884b",
   "metadata": {},
   "source": [
    "General Observations:\n",
    "1. Custom Model: Shows potential in translating basic phrases but requires significant improvements in handling complete sentence structures and contextual accuracy.\n",
    "2. Pre-trained Model: Demonstrates stronger performance overall but still faces challenges with complex sentences and dialogue coherence.\n",
    "\n",
    "The pre-trained model significantly outperforms the custom model, providing more accurate and contextually appropriate translations. The custom model's translations are often incomplete, highlighting the need for further training, fine-tuning, and enhancement of its architecture. The pre-trained model, while generally more reliable, also has areas for improvement, especially in handling complex dialogues and maintaining contextual integrity. These insights underscore the importance of extensive training and fine-tuning to achieve high-quality machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a35f495-3915-4384-b1fc-64cf1bf8487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of custom translations: 500\n",
      "Number of pretrained translations: 500\n",
      "Number of references: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of custom translations: {len(custom_translations)}\")\n",
    "print(f\"Number of pretrained translations: {len(pretrained_translations)}\")\n",
    "print(f\"Number of references: {len(input_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33caf5ba-55b6-4b38-bdc6-2169cf038f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Model BLEU Score: 27.73\n",
      "Pre-trained Model BLEU Score: 54.33\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_metric\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "# Reference translations in the expected format for BLEU calculation\n",
    "references = [[ref] for ref in reference_sentences]  # Ensure using reference sentences in English as required\n",
    "\n",
    "# BLEU score for the custom model translations\n",
    "custom_model_results = bleu.compute(predictions=custom_translations, references=references)\n",
    "custom_bleu_score = custom_model_results['score']\n",
    "print(f\"Custom Model BLEU Score: {custom_bleu_score:.2f}\")\n",
    "\n",
    "\n",
    "# BLEU score for the pre-trained model translations\n",
    "pretrained_model_results = bleu.compute(predictions=pretrained_translations, references=references)\n",
    "pretrained_bleu_score = pretrained_model_results['score']\n",
    "print(f\"Pre-trained Model BLEU Score: {pretrained_bleu_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9ab2c-56bf-45fb-a3d1-6f03c8512232",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Custom Transformer Model: \n",
    "The average BLEU score for the custom transformer model was 27.73, indicating a moderate level of translation accuracy. \n",
    "\n",
    "* Strengths: The model performed well on simple sentences and familiar phrases, demonstrating its ability to learn from the training data. \n",
    "* Weaknesses: The model struggled with complex sentence structures and idiomatic expressions, leading to lower scores on such examples. \n",
    "\n",
    "2. Pre-trained Transformer Model: \n",
    "The average BLEU score for the pre-trained transformer model was 54.33, reflecting a higher level of translation accuracy compared to the custom model. \n",
    "* Strengths: The pre-trained model excelled in handling diverse sentence structures and idiomatic expressions, benefiting from its extensive pre-training on multilingual corpora. \n",
    "* Weaknesses: Although generally robust, the pre-trained model occasionally produced less contextually accurate translations for highly specific domain-related content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdc5d3-5601-47b7-8f74-dc1025593d15",
   "metadata": {},
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81564dbd-ade6-4983-a5b0-6d2054ac09cd",
   "metadata": {},
   "source": [
    "This comparative study showcases the strengths and limitations of both the custom and pre-trained transformer models. The pre-trained model exhibited superior translation quality, as evidenced by higher BLEU scores, while the custom model showed potential in certain contexts. With further fine-tuning and additional training data, the custom model's performance could be enhanced. Overall, the study provides valuable insights into the efficacy of different machine translation approaches, offering a foundation for future improvements and optimizations to achieve more precise and reliable translations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
